NAME
    model.py - Implements a neural network model for Q-learning and a trainer for the reinforcement learning agent.

DESCRIPTION
    This module implements a simple Q-learning agent using a neural network (Linear_QNet) and a trainer (QTrainer). The Linear_QNet class defines a neural network model for approximating Q-values, while the QTrainer class is responsible for training the model using reinforcement learning techniques such as backpropagation. The model and trainer allow for saving and loading of the agent’s state, enabling persistent learning across training sessions.

FUNCTIONS
linear_qnet.save(file_name='model.pth', optimizer=None, n_games=0, epsilon=0)
    Saves the model's state_dict, optimizer state_dict, number of games, and epsilon value to a checkpoint file.
    :param file_name: Name of the file where the model will be saved (default 'model.pth').
    :param optimizer: The optimizer whose state_dict is also saved (default None).
    :param n_games: The number of games played (default 0).
    :param epsilon: The epsilon value representing exploration rate (default 0).
    :return: None.
    
linear_qnet.load(file_name='model.pth', optimizer=None)
    Loads the model’s state_dict, optimizer state_dict (if provided), and returns the number of games and epsilon value from the checkpoint file.
    :param file_name: Name of the file from which to load the model (default 'model.pth').
    :param optimizer: The optimizer whose state_dict is loaded (default None).
    :return: A tuple of (n_games, epsilon), or raises FileNotFoundError if the file is not found.
    
qtrainer.train_step(state, action, reward, next_state, done)
    Performs a training step, calculates the loss, and updates the model using backpropagation.
    :param state: Current state of the environment.
    :param action: Action taken by the agent.
    :param reward: Reward received after taking the action.
    :param next_state: State of the environment after the action.
    :param done: Boolean indicating if the episode has ended.
    :return: None.

CLASSES
Linear_QNet
    Implements a neural network model for approximating Q-values in Q-learning.
    
    Methods defined here:
    
    __init__(self, input_size, hidden_size, output_size)
        Initializes the network layers.
        
        :param input_size: Number of input features.
        :param hidden_size: Number of neurons in the hidden layer.
        :param output_size: Number of possible output actions.
        :return: None.
    
    forward(self, x)
        Forward pass of the network.
        
        :param x: Input tensor.
        :return: Output tensor after passing through the network.
    
    save(self, file_name='model.pth', optimizer=None, n_games=0, epsilon=0)
        Saves the model’s state along with optimizer and other metadata.
        
        :param file_name: The name of the checkpoint file.
        :param optimizer: The optimizer whose state is saved.
        :param n_games: Number of games played.
        :param epsilon: Epsilon value.
        :return: None.
    
    load(self, file_name='model.pth', optimizer=None)
        Loads the model’s state and other metadata from a checkpoint file.
        
        :param file_name: The checkpoint file name.
        :param optimizer: The optimizer whose state is loaded.
        :return: A tuple (n_games, epsilon) or raises FileNotFoundError.

QTrainer
    Trainer class that handles the training of the Linear_QNet using Q-learning principles.
    
    Methods defined here:
    
    __init__(self, model, lr, gamma)
        Initializes the trainer with model, learning rate, and gamma value.
        
        :param model: The model to be trained.
        :param lr: Learning rate for the optimizer.
        :param gamma: Discount factor for future rewards.
        :return: None.
    
    train_step(self, state, action, reward, next_state, done)
        Performs one training step using the provided states, actions, rewards, and done flags.
        
        :param state: The current state.
        :param action: The action taken.
        :param reward: The received reward.
        :param next_state: The next state after taking the action.
        :param done: Boolean flag indicating if the episode is complete.
        :return: None.

DATA
    No global variables or constants are defined in this module.